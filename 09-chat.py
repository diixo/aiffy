# üìå simple_gpt2_chat.py

from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


# 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
model_name = "gpt2"
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)
model.to(device)
model.eval()

# –í–∞–∂–Ω–æ: GPT-2 –Ω–µ –∏–º–µ–µ—Ç pad_token, –∑–∞–¥–∞—ë–º —è–≤–Ω–æ.
tokenizer.pad_token = tokenizer.eos_token  # –¢–µ–ø–µ—Ä—å –ø–∞–¥–¥–∏–Ω–≥ –∏ EOS –æ–¥–∏–Ω–∞–∫–æ–≤—ã.

# 2. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∏—Å—Ç–æ—Ä–∏—é —á–∞—Ç–∞
history = ["System: You are a helpful assistant."]
history = []

print("ü§ñ –ü—Ä–∏–≤–µ—Ç! –≠—Ç–æ GPT-2 —á–∞—Ç-–±–æ—Ç. –ù–∞–ø–∏—à–∏ —á—Ç–æ-—Ç–æ (exit –¥–ª—è –≤—ã—Ö–æ–¥–∞).")

while True:
    user_input = input("–¢—ã: ")
    if user_input.lower() in ["exit", "quit"]:
        print("–ë–æ—Ç: –ü–æ–∫–∞ üëã")
        break

    # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–ø–ª–∏–∫—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    history.append(f"User: {user_input}")
    # –§–æ—Ä–º–∏—Ä—É–µ–º prompt
    prompt = "\n".join(history) + "\nBot:"

    # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Å attention_mask
    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        padding=True
    )

    input_ids = inputs["input_ids"]
    attention_mask = inputs["attention_mask"]

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è
    outputs = model.generate(
        input_ids=input_ids,
        attention_mask=attention_mask,
        max_length=input_ids.shape[1] + 50,
        pad_token_id=tokenizer.eos_token_id,
        do_sample=True,
        top_k=50,
        top_p=0.95,
        temperature=0.7,
        num_return_sequences=1
    )

    # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –≤–µ—Å—å output ‚Üí –±–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–π –∫—É—Å–æ–∫
    generated_text = tokenizer.decode(outputs[0])
    bot_reply = generated_text[len(prompt):].split("\n")[0].strip()

    # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç –±–æ—Ç–∞ –≤ –∏—Å—Ç–æ—Ä–∏—é
    history.append(f"Bot: {bot_reply}")

    print(f"–ë–æ—Ç: {bot_reply}")

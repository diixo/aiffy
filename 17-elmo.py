
# ÐÐ³Ð°, Ñ‚ÐµÐ¿ÐµÑ€ÑŒ Ð¿Ð¾Ð½ÑÑ‚Ð½Ð¾, Ð¾ Ñ‡Ñ‘Ð¼ Ñ‚Ñ‹ ÑÐ¿Ñ€Ð°ÑˆÐ¸Ð²Ð°ÐµÑˆÑŒ ðŸ™‚

# Ð•ÑÐ»Ð¸ Ð¼Ñ‹ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ signature="default" Ð² ELMo v3, Ñ‚Ð¾Ð³Ð´Ð° Ñ‚Ð¾ÐºÐµÐ½Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð´ÐµÐ»Ð°ÐµÑ‚ÑÑ Ð²Ð½ÑƒÑ‚Ñ€Ð¸ ÑÐ°Ð¼Ð¾Ð³Ð¾ TFHub-Ð¼Ð¾Ð´ÑƒÐ»Ñ ELMo, Ð¿Ð¾Ð»Ð½Ð¾ÑÑ‚ÑŒÑŽ ÑÐºÑ€Ñ‹Ñ‚Ð¾ Ð¾Ñ‚ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ:

# Ð’Ñ…Ð¾Ð´: Ð±Ð°Ñ‚Ñ‡ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ð¹ (batch,), ÐºÐ°Ð¶Ð´Ñ‹Ð¹ ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚ â€” ÑÑ‚Ñ€Ð¾ÐºÐ°.

# Ð’Ð½ÑƒÑ‚Ñ€Ð¸ ELMo Ð¿Ñ€Ð¾Ð¸ÑÑ…Ð¾Ð´Ð¸Ñ‚:

# Ð Ð°Ð·Ð±Ð¸ÐµÐ½Ð¸Ðµ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ Ð½Ð° Ñ‚Ð¾ÐºÐµÐ½Ñ‹ (Ð²Ð½ÑƒÑ‚Ñ€ÐµÐ½Ð½Ð¸Ð¹ Ñ‚Ð¾ÐºÐµÐ½Ð¸Ð·Ð°Ñ‚Ð¾Ñ€).

# ÐŸÑ€Ð¾Ñ…Ð¾Ð¶Ð´ÐµÐ½Ð¸Ðµ Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð² Ñ‡ÐµÑ€ÐµÐ· Character-CNN â†’ BiLSTM â†’ Contextual embeddings.

# ÐÐ° Ð²Ñ‹Ñ…Ð¾Ð´Ðµ:

# "default" â†’ ÑƒÑÑ€ÐµÐ´Ð½Ñ‘Ð½Ð½Ñ‹Ð¹ 1024-Ð²ÐµÐºÑ‚Ð¾Ñ€ Ð´Ð»Ñ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ.

# "elmo" (ÐµÑÐ»Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ signature="tokens") â†’ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¸ Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ Ñ‚Ð¾ÐºÐµÐ½Ð°.


import tensorflow as tf
import tensorflow_hub as hub
from keras.layers import Layer


# ---- ELMo ----
class ElmoEmbeddingLayer(Layer):
    def __init__(self, trainable=False, **kwargs):
        super(ElmoEmbeddingLayer, self).__init__(**kwargs)
        self.dimensions = 1024
        self.trainable = trainable

    def build(self, input_shape):
        # Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ ELMo v3 ÐºÐ°Ðº KerasLayer
        self.elmo = hub.KerasLayer(
            "https://tfhub.dev/google/elmo/3",
            #input_shape=[],  # Ð’Ñ…Ð¾Ð´ - Ð¾Ð´Ð¸Ð½ Ñ‚Ð¾ÐºÐµÐ½ Ð¸Ð»Ð¸ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ
            dtype=tf.string,
            trainable=self.trainable,
            #name="{}_module".format(self.name)
            signature="default",
            output_key="elmo",  # pooled embedding
        )
        super(ElmoEmbeddingLayer, self).build(input_shape)


    def call(self, x, mask=None):
        x = tf.squeeze(tf.cast(x, tf.string), axis=1)
        result = self.elmo(x)   # (batch, 1024) or (batch, seq_len, 1024): depend on output_key
        tf.print(">>> result:", result)
        return result


    def compute_mask(self, inputs, mask=None):
        # Ð’ÑÐµ Ñ‚Ð¾ÐºÐµÐ½Ñ‹ Ð²ÐºÐ»ÑŽÑ‡ÐµÐ½Ñ‹ Ð² Ð¼Ð°ÑÐºÑƒ
        mask_tensor = tf.ones_like(inputs, dtype=tf.bool)
        tf.print(">>> compute_mask:", mask_tensor)
        return mask_tensor


if __name__ == "__main__":

    sentences = [
        "I love machine learning very much.",
        "ELMo embeddings are really useful.",
        "TF2 makes life easier."
    ]

    # Transform list from shape [batch, 1] into tf.Tensor
    sentences_tensor = tf.constant(sentences)[:, tf.newaxis]

    # Create embedding layer
    elmo_layer = ElmoEmbeddingLayer(trainable=False)

    # Get embeddings
    embeddings = elmo_layer(sentences_tensor)

    print("Shape:", embeddings.shape)
    # Shape: [3, 1024]

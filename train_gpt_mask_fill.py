
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW
from tqdm import tqdm

# --------- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ----------
MODEL_NAME = "gpt2"  # –∏–ª–∏ "gpt2-medium", "gpt2-xl"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
EPOCHS = 3
BATCH_SIZE = 2
LEARNING_RATE = 5e-5

# --------- –î–∞–Ω–Ω—ã–µ ----------
# –ü—Ä–∏–º–µ—Ä—ã –ø–∞—Ä (input => target)
# –û–±—ã—á–Ω–æ —Ç—ã –±—ã —á–∏—Ç–∞–µ—à—å –∏—Ö –∏–∑ JSONL!
examples = [
    {
        "source": "Fill in: [PERSON] founded [ORG] in [GPE].",
        "target": "Elon Musk founded SpaceX in the United States."
    },
    {
        "source": "Fill in: [PERSON] wrote [WORK].",
        "target": "J.K. Rowling wrote Harry Potter."
    }
]

# --------- –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è ----------
tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)
model = GPT2LMHeadModel.from_pretrained(MODEL_NAME).to(DEVICE)

# GPT –æ–±—ã—á–Ω–æ –Ω–µ –∏–º–µ–µ—Ç BOS/EOS ‚Äî –¥–æ–±–∞–≤–∏–º
tokenizer.pad_token = tokenizer.eos_token

# --------- –ö–∞—Å—Ç–æ–º–Ω—ã–π Dataset ----------
class MaskFillDataset(Dataset):
    def __init__(self, examples, tokenizer, max_length=128):
        self.examples = examples
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.examples)

    def __getitem__(self, idx):
        ex = self.examples[idx]
        # –ü–æ–ª–Ω—ã–π input: prompt + –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
        full_input = ex['source'] + " " + ex['target']
        encoding = self.tokenizer(
            full_input,
            truncation=True,
            max_length=self.max_length,
            padding="max_length",
            return_tensors="pt"
        )
        input_ids = encoding["input_ids"].squeeze()
        attention_mask = encoding["attention_mask"].squeeze()

        # –î–ª—è GPT-2 –º—ã –ø—Ä–æ—Å—Ç–æ —Å–¥–≤–∏–≥–∞–µ–º labels == input_ids
        labels = input_ids.clone()

        return {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "labels": labels
        }

dataset = MaskFillDataset(examples, tokenizer)
loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)

# --------- –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä ----------
optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)

# --------- –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ ----------
model.train()

for epoch in range(EPOCHS):
    loop = tqdm(loader, desc=f"Epoch {epoch+1}")
    for batch in loop:
        input_ids = batch['input_ids'].to(DEVICE)
        attention_mask = batch['attention_mask'].to(DEVICE)
        labels = batch['labels'].to(DEVICE)

        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels
        )
        loss = outputs.loss

        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

        loop.set_postfix(loss=loss.item())

print("‚úÖ Training done!")
#model.save_pretrained("./gpt2_mask_fill_finetuned")
#tokenizer.save_pretrained("./gpt2_mask_fill_finetuned")

##############################################################

tokenizer.pad_token = tokenizer.eos_token
model.eval()  # —Ä–µ–∂–∏–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞

prompt = "Fill in: [PERSON] founded [ORG] in [GPE]."
model.to("cuda" if torch.cuda.is_available() else "cpu")


prompts = [
    "Fill in: [PERSON] founded [ORG] in [GPE].",
    "Fill in: [PERSON] wrote [WORK].",
]

inputs = tokenizer(
    prompts,
    return_tensors="pt",
    padding=True,
    truncation=True)

inputs = {k: v.to(model.device) for k, v in inputs.items()}

output = model.generate(
    input_ids=inputs["input_ids"],
    attention_mask=inputs["attention_mask"],
    max_new_tokens=30,   # —Å–∫–æ–ª—å–∫–æ –º–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω–æ–≤ –≤ –æ—Ç–≤–µ—Ç–µ
#     num_return_sequences=1,
#     do_sample=True,  # –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
#     top_p=0.95,      # nucleus sampling
#     top_k=50         # –∏–ª–∏ –º–æ–∂–Ω–æ top_k=50
# )
    num_beams=3,       # beam search
    do_sample=False,   # beam –¥–µ–ª–∞–µ—Ç –ª—É—á—à–µ –¥–ª—è —Ñ–∞–∫—Ç–æ–≤
    num_return_sequences=1
)

for i, out in enumerate(output):
    full_text = tokenizer.decode(out, skip_special_tokens=True)
    # –£–±–∏—Ä–∞–µ–º prompt –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞, —á—Ç–æ–±—ã –≤–∏–¥–µ—Ç—å —Ç–æ–ª—å–∫–æ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ
    predicted = full_text[len(prompts[i]):].strip()
    print(f"\nüîπ PROMPT: {prompts[i]}\nüîπ OUTPUT: {predicted}")

# decoded = tokenizer.decode(output[0], skip_special_tokens=True)
# print(decoded)

import random
import json

# --- –ü—Ä–∏–º–µ—Ä—ã —Å—É—â–Ω–æ—Å—Ç–µ–π ---
PERSONS = [
    "Elon Musk", "Jeff Bezos", "J.K. Rowling", "Barack Obama",
    "Albert Einstein", "Mark Zuckerberg", "Steve Jobs", "Bill Gates"
]

ORGS = [
    "SpaceX", "Amazon", "Apple", "Microsoft", "Facebook",
    "Tesla", "Google", "OpenAI"
]

GPES = [
    "the United States", "Germany", "Canada", "France", "Japan",
    "the United Kingdom", "Australia"
]

WORKS = [
    "Harry Potter", "The Lord of the Rings", "Pride and Prejudice",
    "A Brief History of Time", "The Hobbit", "Novel-1984"
]

# --- –°–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤ ---
N = 200

# --- –®–∞–±–ª–æ–Ω—ã ---
templates = [
    {
        "source": "Fill-in: [PERSON] founded [ORG] in [GPE].",
        "pattern": "{person} founded {org} in {gpe}."
    },
    {
        "source": "Fill-in: [PERSON] wrote [WORK].",
        "pattern": "{person} wrote {work}."
    },
    {
        "source": "Fill-in: [PERSON] was born in [GPE].",
        "pattern": "{person} was born in {gpe}."
    }
]

# --- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è ---
data = []

for _ in range(N):
    tmpl = random.choice(templates)
    target = tmpl["pattern"].format(
        person=random.choice(PERSONS),
        org=random.choice(ORGS),
        gpe=random.choice(GPES),
        work=random.choice(WORKS)
    )
    example = {
        "source": tmpl["source"],
        "target": target
    }
    data.append(example)


# --- –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ JSONL ---
# with open("mask_fill_corpus.jsonl", "w", encoding="utf-8") as f:
#     for ex in data:
#         f.write(json.dumps(ex) + "\n")

print(f"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {N} –ø—Ä–∏–º–µ—Ä–æ–≤ –≤ mask_fill_corpus.jsonl")


#################################################################

import torch
from torch.utils.data import Dataset, DataLoader
from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW
from tqdm import tqdm

# --------- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ----------
MODEL_NAME = "gpt2"  # –∏–ª–∏ "gpt2-medium", "gpt2-xl"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
EPOCHS = 3
BATCH_SIZE = 8
LEARNING_RATE = 3e-5


examples = data

# --------- –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è ----------
tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)
model = GPT2LMHeadModel.from_pretrained(MODEL_NAME).to(DEVICE)

# GPT –æ–±—ã—á–Ω–æ –Ω–µ –∏–º–µ–µ—Ç BOS/EOS ‚Äî –¥–æ–±–∞–≤–∏–º
tokenizer.pad_token = tokenizer.eos_token

# --------- –ö–∞—Å—Ç–æ–º–Ω—ã–π Dataset ----------
class MaskFillDataset(Dataset):
    def __init__(self, examples, tokenizer, max_length=128):
        self.examples = examples
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.examples)

    def __getitem__(self, idx):
        ex = self.examples[idx]
        # –ü–æ–ª–Ω—ã–π input: prompt + –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
        full_input = ex['source'] + " " + ex['target']
        encoding = self.tokenizer(
            full_input,
            truncation=True,
            max_length=self.max_length,
            padding="max_length",
            return_tensors="pt"
        )
        input_ids = encoding["input_ids"].squeeze()
        attention_mask = encoding["attention_mask"].squeeze()

        # –î–ª—è GPT-2 –º—ã –ø—Ä–æ—Å—Ç–æ —Å–¥–≤–∏–≥–∞–µ–º labels == input_ids
        labels = input_ids.clone()
        labels[input_ids == self.tokenizer.pad_token_id] = -100

        return {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "labels": labels
        }

dataset = MaskFillDataset(examples, tokenizer)
loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)

# --------- –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä ----------
optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)

# --------- –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ ----------
model.train()

for epoch in range(EPOCHS):
    loop = tqdm(loader, desc=f"Epoch {epoch+1}")
    for batch in loop:
        input_ids = batch['input_ids'].to(DEVICE)
        attention_mask = batch['attention_mask'].to(DEVICE)
        labels = batch['labels'].to(DEVICE)

        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels
        )
        loss = outputs.loss

        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

        loop.set_postfix(loss=loss.item())

print("‚úÖ Training done!")
#model.save_pretrained("./gpt2_mask_fill_finetuned")
#tokenizer.save_pretrained("./gpt2_mask_fill_finetuned")

##############################################################

model.eval()  # —Ä–µ–∂–∏–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞

prompt = "Fill-in: [PERSON] founded [ORG] in [GPE]."
model.to("cuda" if torch.cuda.is_available() else "cpu")


prompts = [
    "Fill-in: [PERSON] founded [ORG] in [GPE].",
    "Fill-in: [PERSON] was born in [GPE].",
    "Fill-in: [PERSON] wrote [WORK].",
]

inputs = tokenizer(
    prompts,
    return_tensors="pt",
    padding=True,
    truncation=True)

inputs = {k: v.to(model.device) for k, v in inputs.items()}

output = model.generate(
    input_ids=inputs["input_ids"],
    attention_mask=inputs["attention_mask"],
    max_new_tokens=30,   # —Å–∫–æ–ª—å–∫–æ –º–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω–æ–≤ –≤ –æ—Ç–≤–µ—Ç–µ
    num_return_sequences=1,
    do_sample=True,  # –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
    top_p=0.95,      # nucleus sampling
    top_k=10         # –∏–ª–∏ –º–æ–∂–Ω–æ top_k=50
)
#     num_beams=3,       # beam search
#     do_sample=False,   # beam –¥–µ–ª–∞–µ—Ç –ª—É—á—à–µ –¥–ª—è —Ñ–∞–∫—Ç–æ–≤
#     num_return_sequences=1
# )

for i, out in enumerate(output):
    full_text = tokenizer.decode(out, skip_special_tokens=True)
    # –£–±–∏—Ä–∞–µ–º prompt –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞, —á—Ç–æ–±—ã –≤–∏–¥–µ—Ç—å —Ç–æ–ª—å–∫–æ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ
    predicted = full_text[len(prompts[i]):].strip()
    print(f"\nüîπ PROMPT: {prompts[i]}\nüîπ OUTPUT: {predicted}")

# decoded = tokenizer.decode(output[0], skip_special_tokens=True)
# print(decoded)
